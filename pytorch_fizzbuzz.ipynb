{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FizzBuzz implementation as a multilayer perceptron with pytorch\n",
    "\n",
    "Model based on http://joelgrus.com/2016/05/23/fizz-buzz-in-tensorflow/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:       0\trunning loss: 0.0013862756\tloss: 1.3862756491\n",
      "epoch:    1000\trunning loss: 0.9142203544\tloss: 0.4237289131\n",
      "epoch:    2000\trunning loss: 0.2524000379\tloss: 0.1094664186\n",
      "epoch:    3000\trunning loss: 0.0928775487\tloss: 0.0572104305\n",
      "epoch:    4000\trunning loss: 0.0385754984\tloss: 0.0231338292\n",
      "epoch:    5000\trunning loss: 0.0198464577\tloss: 0.0121519035\n",
      "epoch:    6000\trunning loss: 0.0116750320\tloss: 0.0075830524\n",
      "epoch:    7000\trunning loss: 0.0151903483\tloss: 0.0055109733\n",
      "epoch:    8000\trunning loss: 0.0046500189\tloss: 0.0038128782\n",
      "epoch:    9000\trunning loss: 0.0035680280\tloss: 0.0026144478\n",
      "epoch:   10000\trunning loss: 0.0109901348\tloss: 0.0015988385\n",
      "epoch:   11000\trunning loss: 0.0020416695\tloss: 0.0016909739\n",
      "testing model on numbers from 0 to 100\n",
      "predicted:1 2 fizz 4 buzz fizz 7 8 fizz buzz 11 fizz 13 buzz fizzbuzz 16 17 fizz 19 buzz fizz 22 23 fizz buzz 26 fizz 28 29 fizzbuzz 31 32 fizz 34 buzz fizz 37 buzz fizz buzz 41 42 43 44 fizzbuzz 46 47 fizz 49 buzz fizz 52 53 fizz buzz 56 fizz 58 59 fizzbuzz 61 62 fizz 64 buzz fizz 67 68 fizz buzz 71 fizz 73 74 fizzbuzz 76 77 fizz 79 buzz fizz 82 83 fizz buzz 86 fizz 88 89 fizzbuzz 91 92 fizz 94 buzz fizz 97 buzz fizz \n",
      "failures:\n",
      "i:14\ty_truth:14\ty_pred:buzz\n",
      "i:38\ty_truth:38\ty_pred:buzz\n",
      "i:42\ty_truth:fizz\ty_pred:42\n",
      "i:98\ty_truth:98\ty_pred:buzz\n",
      "Jaccard similarity score: 0.96\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "\n",
    "N_DIGITS = 10\n",
    "TRAINING_RANGE = range(101, 1024)\n",
    "TEST_RANGE = range(1, 100)\n",
    "BATCH_SIZE = 128\n",
    "LEARNING_RATE = 0.01\n",
    "DISPLAY_RATE = 1000\n",
    "\n",
    "PASSES = 1500\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, w_x=N_DIGITS, num_hidden=100, ratio=1):\n",
    "        super(Net, self).__init__()\n",
    "        self.num_hidden = num_hidden\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Linear(w_x, num_hidden),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(int(num_hidden / ratio), 4)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.main(x)\n",
    "\n",
    "\n",
    "def binary_encode(i, num_digits):\n",
    "    return torch.Tensor([i >> d & 1 for d in range(num_digits)])\n",
    "\n",
    "\n",
    "def binary_decode(digits):\n",
    "    return int(sum([(2 ** i) * x for i, x in enumerate(digits)]))\n",
    "\n",
    "\n",
    "def get_class(i):\n",
    "    return int(i % 3 == 0) + int(i % 5 == 0) * 2\n",
    "\n",
    "def get_fizzbuzz(class_, x):\n",
    "    return {0: str(x), 1: \"fizz\", 2: \"buzz\", 3: \"fizzbuzz\"}[class_]\n",
    "\n",
    "def weight_init(m):\n",
    "    if hasattr(m, 'weight'):\n",
    "        m.weight.data.normal_(0.0, 0.01)\n",
    "    if hasattr(m, 'bias'):\n",
    "        m.bias.data.fill_(0)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    net = Net()\n",
    "    net.apply(weight_init)\n",
    "    training_X = torch.stack([binary_encode(i, N_DIGITS) for i in TRAINING_RANGE], 0)\n",
    "    training_Y = torch.LongTensor([[get_class(i)] for i in TRAINING_RANGE])\n",
    "\n",
    "    dataset = TensorDataset(training_X, training_Y)\n",
    "    dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    assert len(training_X) == len(training_Y)\n",
    "\n",
    "    optimizer = optim.Adam(net.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    running_loss = 0.0\n",
    "    net.train()\n",
    "    try:\n",
    "        epoch = 0\n",
    "        for p in range(PASSES):\n",
    "            for i, data in enumerate(dataloader, 0):\n",
    "                x, y = data\n",
    "                x, y = Variable(x), Variable(y.squeeze())\n",
    "\n",
    "                output = net(x)\n",
    "                criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss = criterion(output, y)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                running_loss += float(loss.data)\n",
    "                if epoch % DISPLAY_RATE == 0:\n",
    "                    print(\"epoch:{:8d}\\trunning loss: {:.10f}\\tloss: {:.10f}\" .format(epoch, running_loss / DISPLAY_RATE, float(loss.data)))\n",
    "                    running_loss = 0.0\n",
    "                \n",
    "                epoch = epoch + 1\n",
    "    except KeyboardInterrupt:\n",
    "        pass\n",
    "\n",
    "    net.eval()\n",
    "    true = []\n",
    "    pred = []\n",
    "    print(\"testing model on numbers from 0 to 100\")\n",
    "    print(\"predicted:\", end=\"\")\n",
    "    failures = []\n",
    "    for i, i_bin in [(i, binary_encode(i, N_DIGITS)) for i in TEST_RANGE]:\n",
    "        x = Variable(i_bin)\n",
    "        output = net(x)\n",
    "        true.append(get_class(i))\n",
    "        pred.append(np.argmax(output.data.numpy()))\n",
    "        print(get_fizzbuzz(np.argmax(output.data.numpy()), i) + \" \", end=\"\")\n",
    "        if get_class(i) != np.argmax(output.data.numpy()):\n",
    "            failures.append((i, get_fizzbuzz(get_class(i), i), get_fizzbuzz(np.argmax(output.data.numpy()), i)))\n",
    "    print(\"\")\n",
    "    \n",
    "    print(\"failures:\")\n",
    "    for (i, ytrue, ypred) in failures:\n",
    "          print(\"i:{}\\ty_truth:{}\\ty_pred:{}\".format(i, ytrue, ypred))\n",
    "\n",
    "    print(\"Jaccard similarity score: {:.2f}\".format(metrics.jaccard_similarity_score(true, pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
